2024-04-15:22:42:26,022 INFO     [utils.py:148] Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-04-15:22:42:26,023 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(random_seed=2021, is_training=1, model_id='test', model='Autoformer', data='Close', root_path='./dataset/', data_path='close_data.csv', features='S', target='close_AAPL', freq='h', checkpoints='./checkpoints/', seq_len=7, pred_len=1, label_len=5, bucket_size=4, n_hashes=4, enc_in=1, patch_len=4, stride=8, padding_patch='end', d_model=2, embed_type=0, dec_in=1, c_out=8, n_heads=2, e_layers=1, d_layers=1, d_ff=4, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='fixed', activation='gelu', output_attention=False, do_predict=False, res_attention=True, num_workers=10, itr=1, train_epochs=1, batch_size=256, patience=30, learning_rate=0.001, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)
Use CPU
>>>>>>>start training : test_Autoformer_Close_ftS_sl7_ll5_pl1_dm2_nh2_el1_dl1_df4_fc1_ebfixed_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Epoch: 1 cost time: 2.932508707046509
Epoch: 1, Steps: 63 | Train Loss: 1.0177540 Vali Loss: 0.8644065 Test Loss: 1.0718695
Validation loss decreased (inf --> 0.864407).  Saving model ...
Updating learning rate to 0.001
>>>>>>>testing : test_Autoformer_Close_ftS_sl7_ll5_pl1_dm2_nh2_el1_dl1_df4_fc1_ebfixed_dtTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test shape: (2048, 1, 8) (2048, 1, 1) (2048, 7, 1)
mse:1.0718694925308228, mae:0.8296221494674683
2024-04-15:22:42:42,167 INFO     [utils.py:148] Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-04-15:22:42:42,168 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(random_seed=2021, is_training=1, model_id='test', model='Autoformer', data='Close', root_path='./dataset/', data_path='close_data.csv', features='MS', target='close_AAPL', freq='h', checkpoints='./checkpoints/', seq_len=7, pred_len=1, label_len=5, bucket_size=4, n_hashes=4, enc_in=8, patch_len=4, stride=8, padding_patch='end', d_model=2, embed_type=0, dec_in=8, c_out=8, n_heads=2, e_layers=1, d_layers=1, d_ff=4, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='fixed', activation='gelu', output_attention=False, do_predict=False, res_attention=True, num_workers=10, itr=1, train_epochs=1, batch_size=256, patience=30, learning_rate=0.001, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)
Use CPU
>>>>>>>start training : test_Autoformer_Close_ftMS_sl7_ll5_pl1_dm2_nh2_el1_dl1_df4_fc1_ebfixed_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Epoch: 1 cost time: 2.8593761920928955
Epoch: 1, Steps: 63 | Train Loss: 0.6035952 Vali Loss: 0.4804212 Test Loss: 0.5688553
Validation loss decreased (inf --> 0.480421).  Saving model ...
Updating learning rate to 0.001
>>>>>>>testing : test_Autoformer_Close_ftMS_sl7_ll5_pl1_dm2_nh2_el1_dl1_df4_fc1_ebfixed_dtTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test shape: (2048, 1, 1) (2048, 1, 1) (2048, 7, 8)
mse:0.5688552856445312, mae:0.6102056503295898
